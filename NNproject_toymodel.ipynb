{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"lGMtkLoxpJFx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K8ClOZJrIy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682983825967,"user_tz":240,"elapsed":18868,"user":{"displayName":"Peiqi Chen","userId":"03415428070926034476"}},"outputId":"387385d0-ede0-4720-9b78-551c7f2d491e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"WLTdvXWDv6wA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","import tensorflow as tf\n","\n","minValue = 70\n","\n","def func(path):    \n","    frame = cv2.imread(path)\n","    \n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    blur = cv2.GaussianBlur(gray,(5,5),2)\n","\n","    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n","    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","    return res\n","\n"],"metadata":{"id":"Ancw09gkvCPh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading dataset "],"metadata":{"id":"ek5ovpm6RtxP"}},{"cell_type":"markdown","source":["# Dense model"],"metadata":{"id":"x0iAXLGSB8nl"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Define the image dimensions and batch size\n","image_width, image_height = 200, 200\n","batch_size = 32\n","\n","# Specify the paths to the directories\n","train_dir = '/content/drive/Shareddrives/BA865 - Team Project/Image_dataset/train'\n","validation_dir = '/content/drive/Shareddrives/BA865 - Team Project/Image_dataset/validation'\n","\n","# Create an ImageDataGenerator for data augmentation and rescaling\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Load the training and validation data from the directories\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(image_width, image_height),\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","validation_generator = validation_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size=(image_width, image_height),\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Oqc4s6u5HOt","executionInfo":{"status":"ok","timestamp":1682983851289,"user_tz":240,"elapsed":12419,"user":{"displayName":"Peiqi Chen","userId":"03415428070926034476"}},"outputId":"f0613fe5-d0f0-4a2f-ffc8-ea06ac99e261"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 84 images belonging to 34 classes.\n","Found 34 images belonging to 34 classes.\n"]}]},{"cell_type":"code","source":["# Define the simple dense neural network model\n","model = Sequential([\n","    Flatten(input_shape=(image_width, image_height, 3)),\n","    Dense(128, activation='relu'),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    epochs=10,\n","    validation_data=validation_generator\n",")\n","\n","# Evaluate the model\n","scores = model.evaluate(validation_generator)\n","print(f\"Validation accuracy: {scores[1]*100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8n2SoyZF6IbC","executionInfo":{"status":"ok","timestamp":1682886057376,"user_tz":240,"elapsed":40917,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"610324de-0a96-48bc-d496-1f658069844f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","3/3 [==============================] - 6s 2s/step - loss: 60.2868 - accuracy: 0.0476 - val_loss: 78.8252 - val_accuracy: 0.0294\n","Epoch 2/10\n","3/3 [==============================] - 2s 788ms/step - loss: 64.5286 - accuracy: 0.0476 - val_loss: 56.9160 - val_accuracy: 0.0000e+00\n","Epoch 3/10\n","3/3 [==============================] - 2s 756ms/step - loss: 42.9667 - accuracy: 0.0714 - val_loss: 41.9193 - val_accuracy: 0.0588\n","Epoch 4/10\n","3/3 [==============================] - 3s 1s/step - loss: 31.9483 - accuracy: 0.1548 - val_loss: 31.1110 - val_accuracy: 0.0588\n","Epoch 5/10\n","3/3 [==============================] - 3s 834ms/step - loss: 22.3245 - accuracy: 0.1548 - val_loss: 24.9248 - val_accuracy: 0.0294\n","Epoch 6/10\n","3/3 [==============================] - 2s 871ms/step - loss: 16.8604 - accuracy: 0.1310 - val_loss: 15.8458 - val_accuracy: 0.0294\n","Epoch 7/10\n","3/3 [==============================] - 2s 895ms/step - loss: 9.5141 - accuracy: 0.2143 - val_loss: 10.0977 - val_accuracy: 0.0588\n","Epoch 8/10\n","3/3 [==============================] - 3s 908ms/step - loss: 5.1004 - accuracy: 0.1905 - val_loss: 4.2626 - val_accuracy: 0.0882\n","Epoch 9/10\n","3/3 [==============================] - 4s 1s/step - loss: 3.0201 - accuracy: 0.1905 - val_loss: 4.3514 - val_accuracy: 0.0882\n","Epoch 10/10\n","3/3 [==============================] - 2s 740ms/step - loss: 3.1933 - accuracy: 0.1310 - val_loss: 4.2684 - val_accuracy: 0.0588\n","2/2 [==============================] - 1s 47ms/step - loss: 4.2684 - accuracy: 0.0588\n","Validation accuracy: 5.88%\n"]}]},{"cell_type":"markdown","source":["## Prediction on testset "],"metadata":{"id":"6w-H-WgWRyJW"}},{"cell_type":"code","source":["import os\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import numpy as np\n","\n","def predict_single_image(model, image_path, target_size, class_labels):\n","    image = load_img(image_path, target_size=target_size)\n","    image_array = img_to_array(image)\n","    image_array = np.expand_dims(image_array, axis=0)\n","    prediction = model.predict(image_array)\n","    predicted_class_index = np.argmax(prediction, axis=-1)[0]\n","    predicted_class_label = class_labels[predicted_class_index]\n","    return predicted_class_label\n","\n","def predict_multiple_images(model, folder_path, class_labels, target_size=(200, 200)):\n","    predicted_labels = {}\n","    for root, dirs, files in os.walk(folder_path):\n","        for image_filename in files:\n","            # Construct the full path to the image file\n","            image_path = os.path.join(root, image_filename)\n","            if os.path.isfile(image_path):  # Ensure that the item is a file\n","                predicted_class_label = predict_single_image(model, image_path, target_size, class_labels)\n","                predicted_labels[image_filename] = predicted_class_label\n","    return predicted_labels\n","\n","# Define the path to the folder containing the test images\n","test_dir = '/content/drive/Shareddrives/BA865 - Team Project/Image_dataset/test'\n","\n","# Get the class labels from the training generator\n","class_labels = list(train_generator.class_indices.keys())\n","\n","# Use the trained model to predict the classes of the test images\n","predicted_labels = predict_multiple_images(model, test_dir, class_labels)\n","print('Predicted class labels for test images:')\n","for image_filename, predicted_class_label in predicted_labels.items():\n","    print(f'{image_filename}: {predicted_class_label}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFMiEGjGP8PO","executionInfo":{"status":"ok","timestamp":1682886057555,"user_tz":240,"elapsed":202,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"8dfa93b7-a4c6-4bb7-9d88-a169005dd179"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 35ms/step\n","Predicted class labels for test images:\n","yes.0.png: phone\n","eat.0.png: phone\n"]}]},{"cell_type":"markdown","source":["## Pre-train cnn"],"metadata":{"id":"oHznYn_vp8nx"}},{"cell_type":"code","source":["from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n","\n","# Load the VGG16 pretrained model without the top dense layers\n","conv_base = VGG16(\n","    weights=\"imagenet\",\n","    include_top=False,  # Exclude the top dense layers\n","    input_shape=(200, 200, 3)  # The input shape for the base model (grayscale images will need to be converted to RGB)\n",")\n","\n","# Get the output of the base model\n","x = conv_base.output\n","\n","# Add a global average pooling layer\n","x = GlobalAveragePooling2D()(x)\n","\n","# Add a dropout layer (optional)\n","x = Dropout(0.25)(x)\n","\n","# Add a dense layer with 128 neurons and ReLU activation\n","x = Dense(128, activation='relu')(x)\n","\n","# Add a dropout layer with 50% dropout rate\n","x = Dropout(0.5)(x)\n","\n","# Output layer with the number of neurons equal to the number of classes, using softmax activation\n","num_classes = train_generator.num_classes  # Number of classes to predict\n","predictions = Dense(num_classes, activation='softmax')(x)\n","\n","# Create a new model combining the base model and custom layers\n","model = Model(inputs=conv_base.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print a summary of the model architecture\n","model.summary()\n","\n","# Train the model using the data generators\n","history = model.fit(\n","    train_generator,\n","    epochs=10,\n","    validation_data=validation_generator\n",")\n","\n","# Evaluate the model on the validation data\n","scores = model.evaluate(validation_generator)\n","print(f\"Validation accuracy: {scores[1]*100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bPlfy8vCp_hE","executionInfo":{"status":"error","timestamp":1682985112475,"user_tz":240,"elapsed":1084159,"user":{"displayName":"Peiqi Chen","userId":"03415428070926034476"}},"outputId":"19574e9b-aac0-48ba-ea56-85e197068c8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 200, 200, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 200, 200, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 200, 200, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 100, 100, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 100, 100, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 100, 100, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 50, 50, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 50, 50, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 50, 50, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 50, 50, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 25, 25, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 25, 25, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 25, 25, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 25, 25, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 12, 12, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 12, 12, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense (Dense)               (None, 128)               65664     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 34)                4386      \n","                                                                 \n","=================================================================\n","Total params: 14,784,738\n","Trainable params: 14,784,738\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","3/3 [==============================] - 192s 74s/step - loss: 18.1869 - accuracy: 0.0119 - val_loss: 4.1889 - val_accuracy: 0.0294\n","Epoch 2/10\n","3/3 [==============================] - 175s 57s/step - loss: 4.4601 - accuracy: 0.0238 - val_loss: 3.5653 - val_accuracy: 0.0294\n","Epoch 3/10\n","3/3 [==============================] - 176s 58s/step - loss: 3.6080 - accuracy: 0.0119 - val_loss: 3.5326 - val_accuracy: 0.0588\n","Epoch 4/10\n","3/3 [==============================] - 193s 67s/step - loss: 3.4711 - accuracy: 0.0238 - val_loss: 3.5530 - val_accuracy: 0.0588\n","Epoch 5/10\n","3/3 [==============================] - 175s 58s/step - loss: 3.6503 - accuracy: 0.0000e+00 - val_loss: 3.5401 - val_accuracy: 0.0588\n","Epoch 6/10\n","1/3 [=========>....................] - ETA: 1:58 - loss: 3.6300 - accuracy: 0.0000e+00"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3219f780fcd4>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train the model using the data generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Conv2 model"],"metadata":{"id":"o664Pzo8Ig17"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n","\n","# Define the CNN model\n","model = Sequential([\n","    # Convolutional layer with 32 filters, each with size 3x3, and ReLU activation\n","    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(200, 200, 3)),\n","    # Max pooling layer with pool size 2x2\n","    MaxPooling2D(pool_size=(2, 2)),\n","    # Dropout layer with 25% dropout rate\n","    Dropout(0.25),\n","    # Another convolutional layer with 64 filters, each with size 3x3, and ReLU activation\n","    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n","    # Max pooling layer with pool size 2x2\n","    MaxPooling2D(pool_size=(2, 2)),\n","    # Dropout layer with 25% dropout rate\n","    Dropout(0.25),\n","    # Flatten layer to convert 3D feature maps to 1D feature vectors\n","    Flatten(),\n","    # Dense layer with 128 neurons and ReLU activation\n","    Dense(128, activation='relu'),\n","    # Dropout layer with 50% dropout rate\n","    Dropout(0.5),\n","    # Output layer with the number of neurons equal to the number of classes, using softmax activation\n","    Dense(len(train_generator.class_indices), activation='softmax')  # num_classes should be replaced with the actual number of classes\n","])\n"],"metadata":{"id":"4oX3kQRwJFKu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print a summary of the model architecture\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjmL9xdYKwzD","executionInfo":{"status":"ok","timestamp":1682886057900,"user_tz":240,"elapsed":163,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"a16e8b15-cc1d-45e5-a310-0f2b25b245b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 198, 198, 32)      896       \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 99, 99, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 99, 99, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 97, 97, 64)        18496     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 48, 48, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 48, 48, 64)        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 147456)            0         \n","                                                                 \n"," dense_6 (Dense)             (None, 128)               18874496  \n","                                                                 \n"," dropout_5 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_7 (Dense)             (None, 34)                4386      \n","                                                                 \n","=================================================================\n","Total params: 18,898,274\n","Trainable params: 18,898,274\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["\n","# Train the model (replace train_generator and validation_generator with your actual generators)\n","history = model.fit(\n","    train_generator,\n","    epochs=20,\n","    validation_data=validation_generator\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxLKz86PK0fR","executionInfo":{"status":"ok","timestamp":1682886567402,"user_tz":240,"elapsed":254793,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"8a63087b-542b-408e-a0a2-694746cbd779"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","3/3 [==============================] - 11s 3s/step - loss: 3.2691 - accuracy: 0.1071 - val_loss: 3.5304 - val_accuracy: 0.0588\n","Epoch 2/20\n","3/3 [==============================] - 9s 3s/step - loss: 3.0898 - accuracy: 0.1310 - val_loss: 3.4957 - val_accuracy: 0.0882\n","Epoch 3/20\n","3/3 [==============================] - 9s 3s/step - loss: 3.0482 - accuracy: 0.1310 - val_loss: 3.4744 - val_accuracy: 0.1176\n","Epoch 4/20\n","3/3 [==============================] - 11s 4s/step - loss: 2.8286 - accuracy: 0.1905 - val_loss: 3.4524 - val_accuracy: 0.1176\n","Epoch 5/20\n","3/3 [==============================] - 9s 3s/step - loss: 2.7506 - accuracy: 0.2024 - val_loss: 3.4105 - val_accuracy: 0.1176\n","Epoch 6/20\n","3/3 [==============================] - 9s 3s/step - loss: 2.6816 - accuracy: 0.2143 - val_loss: 3.3538 - val_accuracy: 0.1471\n","Epoch 7/20\n","3/3 [==============================] - 11s 4s/step - loss: 2.3068 - accuracy: 0.3452 - val_loss: 3.3734 - val_accuracy: 0.1765\n","Epoch 8/20\n","3/3 [==============================] - 11s 4s/step - loss: 2.2895 - accuracy: 0.3452 - val_loss: 3.2975 - val_accuracy: 0.1471\n","Epoch 9/20\n","3/3 [==============================] - 9s 3s/step - loss: 2.1910 - accuracy: 0.3571 - val_loss: 3.3245 - val_accuracy: 0.1176\n","Epoch 10/20\n","3/3 [==============================] - 11s 3s/step - loss: 2.0073 - accuracy: 0.4405 - val_loss: 3.3391 - val_accuracy: 0.1765\n","Epoch 11/20\n","3/3 [==============================] - 10s 4s/step - loss: 1.9556 - accuracy: 0.4405 - val_loss: 3.3310 - val_accuracy: 0.1471\n","Epoch 12/20\n","3/3 [==============================] - 9s 3s/step - loss: 1.7627 - accuracy: 0.5476 - val_loss: 3.3992 - val_accuracy: 0.1471\n","Epoch 13/20\n","3/3 [==============================] - 11s 4s/step - loss: 1.6491 - accuracy: 0.4762 - val_loss: 3.4235 - val_accuracy: 0.1765\n","Epoch 14/20\n","3/3 [==============================] - 9s 3s/step - loss: 1.6158 - accuracy: 0.5357 - val_loss: 3.5107 - val_accuracy: 0.1765\n","Epoch 15/20\n","3/3 [==============================] - 11s 3s/step - loss: 1.4222 - accuracy: 0.5595 - val_loss: 3.3401 - val_accuracy: 0.1471\n","Epoch 16/20\n","3/3 [==============================] - 11s 4s/step - loss: 1.3718 - accuracy: 0.5714 - val_loss: 3.6301 - val_accuracy: 0.1471\n","Epoch 17/20\n","3/3 [==============================] - 10s 4s/step - loss: 1.2228 - accuracy: 0.6429 - val_loss: 3.6383 - val_accuracy: 0.1765\n","Epoch 18/20\n","3/3 [==============================] - 11s 4s/step - loss: 1.1742 - accuracy: 0.6429 - val_loss: 3.6567 - val_accuracy: 0.1765\n","Epoch 19/20\n","3/3 [==============================] - 10s 4s/step - loss: 1.1422 - accuracy: 0.6548 - val_loss: 3.5086 - val_accuracy: 0.1176\n","Epoch 20/20\n","3/3 [==============================] - 10s 3s/step - loss: 1.1347 - accuracy: 0.6429 - val_loss: 3.6007 - val_accuracy: 0.1471\n"]}]},{"cell_type":"code","source":["\n","# Evaluate the model on the validation data\n","scores = model.evaluate(validation_generator)\n","print(f\"Validation accuracy: {scores[1]*100:.2f}%\")\n"],"metadata":{"id":"HfqyUqG6Kqto","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682886201721,"user_tz":240,"elapsed":2269,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"9ed62309-0f86-426a-cc8f-616c937bfd21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 2s 63ms/step - loss: 3.5495 - accuracy: 0.0588\n","Validation accuracy: 5.88%\n"]}]},{"cell_type":"markdown","source":["## Prediction on testset"],"metadata":{"id":"-5b0x8p8R5Du"}},{"cell_type":"code","source":["import os\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import numpy as np\n","\n","def predict_single_image(model, image_path, target_size, class_labels):\n","    image = load_img(image_path, target_size=target_size)\n","    image_array = img_to_array(image)\n","    image_array = np.expand_dims(image_array, axis=0)\n","    prediction = model.predict(image_array)\n","    predicted_class_index = np.argmax(prediction, axis=-1)[0]\n","    predicted_class_label = class_labels[predicted_class_index]\n","    return predicted_class_label\n","\n","def predict_multiple_images(model, folder_path, class_labels, target_size=(200, 200)):\n","    predicted_labels = {}\n","    for root, dirs, files in os.walk(folder_path):\n","        for image_filename in files:\n","            # Construct the full path to the image file\n","            image_path = os.path.join(root, image_filename)\n","            if os.path.isfile(image_path):  # Ensure that the item is a file\n","                predicted_class_label = predict_single_image(model, image_path, target_size, class_labels)\n","                predicted_labels[image_filename] = predicted_class_label\n","    return predicted_labels\n","\n","# Define the path to the folder containing the test images\n","test_dir = '/content/drive/Shareddrives/BA865 - Team Project/Image_dataset/test'\n","\n","# Get the class labels from the training generator (replace train_generator with your actual generator)\n","class_labels = list(train_generator.class_indices.keys())\n","\n"],"metadata":{"id":"Qr5-he6aPMHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the trained model to predict the classes of the test images\n","predicted_labels = predict_multiple_images(model, test_dir, class_labels)\n","print('Predicted class labels for test images:')\n","for image_filename, predicted_class_label in predicted_labels.items():\n","    print(f'{image_filename}: {predicted_class_label}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfv0MI4dPQkk","executionInfo":{"status":"ok","timestamp":1682886202157,"user_tz":240,"elapsed":310,"user":{"displayName":"Xinyuan Xu","userId":"06628972086129009781"}},"outputId":"55298de6-76df-4d2f-fb52-cc0ca7c7256b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 113ms/step\n","1/1 [==============================] - 0s 58ms/step\n","Predicted class labels for test images:\n","yes.0.png: rain\n","eat.0.png: eat\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4BQB2yXGpjdN"},"execution_count":null,"outputs":[]}]}